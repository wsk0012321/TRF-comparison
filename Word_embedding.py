{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2239afe6-ba37-4f80-9b71-31365499b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import transformers\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import scipy.io as sio\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "word2vec = gensim.models.Word2Vec.load(r'E:/PhD/data/Di_Liberto/word2vec_bnc/word2vec_bnc_400d.model')\n",
    "\n",
    "def reconstruct_sents(offsets,words,boundaries):\n",
    "\n",
    "    values = sorted(list(set(offsets + boundaries)))\n",
    "    padding = [0] + [1 if val in offsets \n",
    "               else 0\n",
    "              for val in values] + [0] # add one more zero at the end in case that the list ends with 1; add one more zero at beginning in case that the list starts with 1\n",
    "    \n",
    "    # retrieve indices of valid values\n",
    "    valid_idx = []\n",
    "    for i in range(len(values)-1): # to avoid out of range\n",
    "        curr_value = values[i]\n",
    "        # if the current padding is one and the next is zero\n",
    "        if padding[i+1] == 1 and padding[i+2] == 0:\n",
    "            valid_idx.append(offsets.index(curr_value))\n",
    "     \n",
    "    valid_idx += [offsets.index(val) for val in offsets if val in boundaries]\n",
    "    valid_idx = sorted(list(set(valid_idx)))\n",
    "    offset_sent_final = [offsets[idx] for idx in valid_idx]\n",
    "    \n",
    "    init_id = 0\n",
    "    sent_list = []\n",
    "    for idx in valid_idx:\n",
    "        sent_list.append(words[init_id:idx+1])\n",
    "        init_id = idx+1\n",
    "    \n",
    "    return offset_sent_final, sent_list\n",
    "\n",
    "def process_stim(data):\n",
    "    \n",
    "    boundaries = [round(float(val),2) for val in data['sentence_boundaries'][0]]\n",
    "    onsets = [float(val[0]) for val in data['onset_time']]\n",
    "    offsets = [float(val[0]) for val in data['offset_time']]\n",
    "    words =[str(val[0][0]).strip() for val in data['wordVec']]\n",
    "    \n",
    "    offsets_sf, sent_list = reconstruct_sents(offsets, words, boundaries)\n",
    "    \n",
    "    return offsets_sf, sent_list, boundaries, onsets\n",
    "\n",
    "def word2vec_vals(sent_list):\n",
    "    \n",
    "    dissi_vals = []\n",
    "    \n",
    "    for i in range(len(sent_list)):\n",
    "        if i == 0:\n",
    "            for n in range(len(sent_list[0])):\n",
    "                if n == 0:\n",
    "                    dissi_vals.append(0)\n",
    "                else:\n",
    "                    target_vec = word2vec.wv[sent_list[i][n]]\n",
    "                    vec_list = [word2vec.wv[w] for w in sent_list[i][:n]]\n",
    "                    averaged_vec = sum(vec_list) / len(vec_list)\n",
    "                    #corr = cosine(target_vec,averaged_vec)\n",
    "                    corr,_ = pearsonr(target_vec,averaged_vec)\n",
    "                    dissi_vals.append(1-corr)\n",
    "        \n",
    "        else:        \n",
    "            for n in range(len(sent_list[i])):\n",
    "                target_vec = word2vec.wv[sent_list[i][n]]\n",
    "                vec_list = [word2vec.wv[w] for w in sent_list[i-1]] if n == 0 else [word2vec.wv[w] for w in sent_list[i][:n]]\n",
    "                averaged_vec = sum(vec_list) / len(vec_list)\n",
    "                #corr = cosine(target_vec,averaged_vec)\n",
    "                corr,_ = pearsonr(target_vec,averaged_vec)\n",
    "                dissi_vals.append(1-corr)\n",
    "                                  \n",
    "    return dissi_vals\n",
    "\n",
    "def match_tokens(words,encodings):\n",
    "    \n",
    "    word_list = [w.lower() for w in words]\n",
    "    matches = {}\n",
    "    input_ids = [int(val) for val in encodings['input_ids'][0]]\n",
    "    \n",
    "    multi_token = False\n",
    "    accum = ''\n",
    "    init_id = 0\n",
    "    w_id = 0\n",
    "    \n",
    "    for i in range(len(input_ids)):\n",
    "        \n",
    "        \n",
    "        word = word_list[w_id]\n",
    "        token_id = input_ids[i]\n",
    "       \n",
    "        # reconstruct the token\n",
    "            \n",
    "        token = re.search(\"[\\w\\d\\']+\",tokenizer.convert_ids_to_tokens(token_id)).group()\n",
    "       \n",
    "        if token == word and multi_token == False:\n",
    "            # update matches\n",
    "            matches[token] = [i]\n",
    "            # update initial index\n",
    "            init_id += 1\n",
    "            w_id += 1\n",
    "        else: \n",
    "            multi_token = True\n",
    "            accum += token\n",
    "            # check if the new token matches a word\n",
    "            if accum == word:\n",
    "                matches[accum] = list(range(init_id,(i+1)))\n",
    "                # reset the flags\n",
    "                multi_token = False\n",
    "                accum = ''\n",
    "                # update init_id\n",
    "                init_id = i+1\n",
    "                # only when a word is matched, will we go on to check the next word\n",
    "                w_id += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    return matches\n",
    "            \n",
    "def bert_vals(sent_list):\n",
    "    \n",
    "    def retrieve_vec(token_range, vectors):\n",
    "        if len(token_range) == 1:\n",
    "            word_onset = token_range[0]\n",
    "            target_vec = vectors[word_onset].tolist()\n",
    "        else:\n",
    "            word_onset = token_range[0]\n",
    "            word_coda = token_range[-1]\n",
    "            # if a word has more than one tokens, we apply pooling\n",
    "            target_vec = torch.mean(vectors[word_onset:word_coda],dim=0).tolist()\n",
    "            \n",
    "        return word_onset, target_vec\n",
    "    \n",
    "    def cosine_sim(target_vec, vec_list):\n",
    "        \n",
    "        averaged_vec = torch.mean(vec_list,dim=0).tolist()\n",
    "        corr= cosine(target_vec,averaged_vec)\n",
    "        \n",
    "        return corr\n",
    "    \n",
    "    def pearson_corr(target_vec, vec_list):\n",
    "        \n",
    "        averaged_vec = torch.mean(vec_list,dim=0).tolist()\n",
    "        corr,_ = pearsonr(target_vec,averaged_vec)\n",
    "        \n",
    "        return corr\n",
    "        \n",
    "    dissi_vals = []\n",
    "    for i in range(len(sent_list)):\n",
    "        \n",
    "        token_list = [t.lower() for t in sent_list[i]]\n",
    "        sent = ' '.join(token_list)\n",
    "        encodings = tokenizer(sent,add_special_tokens=False, return_tensors='pt')\n",
    "        matches = match_tokens(token_list, encodings)\n",
    "        vectors = encoder(**encodings).last_hidden_state[0]\n",
    "\n",
    "        if i == 0:\n",
    "            for n in range(len(token_list)):\n",
    "                if n == 0:\n",
    "                    dissi_vals.append(0)\n",
    "                else:\n",
    "                    # retrieve the idx or idx range\n",
    "                    token_range = matches[token_list[n]]\n",
    "                    word_onset, target_vec = retrieve_vec(token_range,vectors)\n",
    "                    # retrieve the vectors of the context\n",
    "                    vec_list = vectors[:word_onset]\n",
    "                    # compute correlation value\n",
    "                    #corr = cosine_sim(target_vec,vec_list)\n",
    "                    corr = pearson_corr(target_vec,vec_list)\n",
    "                    dissi_vals.append(1-corr)\n",
    "                    \n",
    "        else:\n",
    "            for n in range(len(token_list)):\n",
    "                token_range = matches[token_list[n]]\n",
    "                word_onset, target_vec = retrieve_vec(token_range,vectors)\n",
    "        \n",
    "                vec_list = vectors[:word_onset] if n != 0 else encoder(**tokenizer(' '.join(sent_list[i-1]),add_special_tokens=False,return_tensors='pt')).last_hidden_state[0]\n",
    "                #corr = cosine_sim(target_vec,vec_list)\n",
    "                corr = pearson_corr(target_vec, vec_list)\n",
    "                \n",
    "                dissi_vals.append(1-corr)\n",
    "                                  \n",
    "    return dissi_vals\n",
    "            \n",
    "def calculate_dissi(data):\n",
    "                                  \n",
    "    offsets_sf, sent_list, boundaries, onsets = process_stim(data)\n",
    "    w2v = word2vec_vals(sent_list)\n",
    "    bert = bert_vals(sent_list)\n",
    "                                  \n",
    "    # align the onsets and values\n",
    "    return list(zip(onsets,w2v,bert))\n",
    "\n",
    "def align_with_eeg(data):\n",
    "    \n",
    "    expanded_data = []\n",
    "    time_points = [row[1].to_list() for row in data.iterrows()]\n",
    "    \n",
    "    onset_id = 0\n",
    "    coda_id = 0\n",
    "    for p in time_points:\n",
    "        if p[1] == p[2] == 0 and coda_id != 0:\n",
    "            sub_data = time_points[onset_id+1:coda_id]\n",
    "            onset_id = coda_id\n",
    "            expanded_data.append(expand_data(sub_data))\n",
    "        \n",
    "        coda_id += 1\n",
    "        \n",
    "    return expanded_data\n",
    "\n",
    "\n",
    "def expand_data(data):\n",
    "    \n",
    "    sample_points = [num/128 for num in range(22912)]\n",
    "    output = [0] * 22912\n",
    "    \n",
    "    for p in data:\n",
    "        closest = min(sample_points, key = lambda x: abs(x-p[0]))\n",
    "        index = sample_points.index(closest)\n",
    "        output[index] = (p[1],p[2])\n",
    "    \n",
    "    for i in range(1,22912):\n",
    "        if output[i] == 0:\n",
    "            output[i] = output[i-1]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return output\n",
    "\n",
    "paths = glob(r'E:/PhD/data/Di_Liberto/Di_Liberto/Natural Speech/Stimuli/Text/*.mat')\n",
    "# four datasets are removed because they contain words that cannot be processed by the word2vec model\n",
    "paths.pop(16)\n",
    "paths.pop(15)\n",
    "paths.pop(9)\n",
    "paths.pop(8)\n",
    "\n",
    "onset_vals = []\n",
    "for i,p in enumerate(paths):\n",
    "    start_time = time()\n",
    "    data = sio.loadmat(p)\n",
    "    onset_vals.append(calculate_dissi(data))\n",
    "    end_time = time()\n",
    "    print('--------------------------------------------------------------------------------------------------')\n",
    "    print(f'file {i} completed; time: {end_time-start_time}')\n",
    "    \n",
    "data = []\n",
    "for x in onset_vals:\n",
    "    for s in x:\n",
    "        data.append(s)\n",
    "df = pd.DataFrame.from_records(data,columns=['onset','w2v','bert'])\n",
    "\n",
    "expanded_data = align_with_eeg(df)\n",
    "#with open(r'E:/PhD/data/Di_Liberto/value_cosine.pkl','wb') as file:\n",
    "    #pickle.dump(expanded_data, file)\n",
    "with open(r'E:/PhD/data/Di_Liberto/value_pearson.pkl','wb') as file:\n",
    "    #pickle.dump(expanded_data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
